{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines solvers\n",
    "\n",
    "The objective is to solve the **primal** problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w \\in \\mathbb{R}^n,\\, z \\in \\mathbb{R}^m}  & \\quad \\frac{1}{2} ||w||_2^2 + C \\mathbb{1}^T z \\\\\n",
    "\\text{subject to } & \\quad y_i(w^T x_i) \\geq 1-z_i, \\quad i = 1,...,m \\\\\n",
    "& \\quad z \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We define $f(w, z) = \\frac{1}{2} ||w||_2^2 + C \\mathbb{1}^T z$ the objective function and, for $i = 1,...,m$, $g_i(w, z) = 1-z_i-y_i(w^T x_i)$ and $h_i(w, z) = -z_i$ the inequality constraint functions.\n",
    "\n",
    "We can rewrite the optimization problem using these functions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,\\, z} & \\quad f(w, z) \\\\\n",
    "\\text{subject to } & \\quad g_i(w, z) \\leq 0, \\quad i = 1,...,m \\\\\n",
    "& \\quad h_i(w, z) \\leq 0, \\quad i = 1,...,m\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note :** A vector $p \\in \\mathbb{R}^k$ is represented by a one-column matrix with $k$ lines. For making notations easy to write, we represent the vertical concatenation of $p \\in \\mathbb{R}^k$ and $q \\in \\mathbb{R}^l$ by $[p \\, |\\, q]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal and dual approximations via logarithmic barrier\n",
    "\n",
    "### Primal approximation\n",
    "\n",
    "We approximate the primal problem by this problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,\\, z} & \\quad f(w, z) - \\frac{1}{t}\\left(\\sum_{i = 1}^m \\log(-g_i(w, z)) +  \\sum_{i = 1}^m \\log(-h_i(w, z))\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To optimize it, we will need the following gradients:\n",
    "\n",
    "- $\\nabla f(w, z) = [w\\,|\\,C\\mathbb{1}]$\n",
    "- $\\nabla g_i(w, z) = -[y_i x_i\\,|\\,\\mathbb{1}_i]$ where $\\mathbb{1}_i$ is the vector with a $1$ at position $i$ and zeros otherwise\n",
    "- $\\nabla h_i(w, z) = -[\\mathbb{0}\\,|\\,\\mathbb{1}_i]$\n",
    "\n",
    "and the following hessians:\n",
    "\n",
    "- $\\nabla^2 f(w, z) = \\left( \\begin{array}{cc}\n",
    "I_n & O \\\\\n",
    "O & O \\end{array} \\right)$\n",
    "- $\\nabla^2 g_i(w, z) = O$\n",
    "- $\\nabla^2 h_i(w, z) = O$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual approximation\n",
    "\n",
    "Before approximating the dual, we need to find the **dual** problem. We define the laplacian:\n",
    "$$L(w, z, u, v) = f(w, z) + \\sum_{i = 1}^m u_i g_i(w, z) + \\sum_{i = 1}^m v_i h_i(w, z).$$\n",
    "As $\\nabla^2_{w, z} L(w, z, u, v) = \\left( \\begin{array}{cc}\n",
    "I_n & O \\\\\n",
    "O & O \\end{array} \\right) \\succcurlyeq 0$, $(w, z) \\mapsto L(w, z, u, v)$ is convex. In order to minimize $L(w, z, u, v)$ in $w$ and $z$, we just need to find $w$ and $z$ such that:\n",
    "\n",
    "- $\\nabla_w L(w, z, u, v) = w - \\sum_{i = 1}^{m} u_i y_i x_i = \\mathbb{0}$\n",
    "- $\\nabla_z L(w, z, u, v) = C \\mathbb{1} - u - v = \\mathbb{0}$.\n",
    "\n",
    "Then, we get the dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{u \\in \\mathbb{R}^m}  & \\quad - \\frac{1}{2} ||Ku||_2^2 + \\mathbb{1}^T u \\\\\n",
    "\\text{subject to } & \\quad -u \\leq 0 \\\\\n",
    "& \\quad u - C\\mathbb{1}\\leq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $K = \\left( y_1 x_1 \\,|\\, ... \\,|\\, y_m x_m \\right)$.\n",
    "\n",
    "We define $r(u) = - \\frac{1}{2} ||Ku||_2^2 + \\mathbb{1}^T u$ the objective function and, for $i = 1,...,m$, $s_i(u) = -u_i$ and $t_i(u) = u_i-C$ the inequality constraint functions.\n",
    "\n",
    "We approximate the dual problem by this problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{u} & \\quad r(u) - \\frac{1}{t}\\left(\\sum_{i = 1}^m \\log(-s_i(w, z)) +  \\sum_{i = 1}^m \\log(-t_i(w, z))\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To optimize it, we will need the following gradients:\n",
    "\n",
    "- $\\nabla r(u) = - K^T K u + \\mathbb{1}$\n",
    "- $\\nabla s_i(u) = -\\mathbb{1}_i$\n",
    "- $\\nabla t_i(u) = \\mathbb{1}_i$\n",
    "\n",
    "and the following hessians:\n",
    "\n",
    "- $\\nabla^2 r(u) = - K^T K$\n",
    "- $\\nabla^2 s_i(u) = O$\n",
    "- $\\nabla^2 t_i(u) = O$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal and dual optimizations via logarithmic barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logarithmic_barrier_approx(t, Fs, Fps, Fpps):\n",
    "    \"\"\"\n",
    "    Computes the logarithmic barrier approximation function\n",
    "    of a convex optimization problem, its gradient and its hessian\n",
    "    (formulas in slide 74, course 2).\n",
    "    \n",
    "    Args:\n",
    "        t: the approximation quality\n",
    "        Fs: the list with the objective function first and the\n",
    "            inequality constraint functions next\n",
    "        Fps: the list with the gradients of the objective function first\n",
    "             and the inequality constraint functions next\n",
    "        Fpps: the list with the hessians of the objective function first\n",
    "              and the inequality constraint functions next\n",
    "    \n",
    "    Returns:\n",
    "        The logarithmic barrier approximation function, its gradient\n",
    "        and its hessian.\n",
    "    \"\"\"\n",
    "    def LF(x):\n",
    "        r = Fs[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(-np.log(-Fs[i](x)))\n",
    "        return r\n",
    "    \n",
    "    def LFp(x):\n",
    "        r = Fps[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(-1/Fs[i](x)*Fps[i](x))\n",
    "        return r\n",
    "    \n",
    "    def LFpp(x):\n",
    "        r = Fpps[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(1/Fs[i](x)**2*Fps[i](x).dot(Fps[i](x).T)-1/Fs[i](x)*Fpps[i](x)) \n",
    "        return r\n",
    "    \n",
    "    return LF, LFp, LFpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking line search parameters\n",
    "alpha = 1/2\n",
    "beta = 3/4\n",
    "\n",
    "def newton(F, Fp, Fpp, x0, eps):\n",
    "    \"\"\"\n",
    "    Minimizes F using Newton's method (algorithm in slide 20, course 2).\n",
    "    \n",
    "    Args:\n",
    "        F: a function from R^n to R\n",
    "        Fp: the gradient of F\n",
    "        Fpp: the hessian of F\n",
    "        x0: the initial point\n",
    "        eps: the precision of the approximation\n",
    "    \n",
    "    Returns:\n",
    "        The sequence of approximations of the extremum of F up to a\n",
    "        precision eps. Useful for debugging.\n",
    "    \"\"\"\n",
    "    def get_next_x(x):\n",
    "        Fpx = Fp(x)\n",
    "        delta_x = -np.linalg.inv(Fpp(x)).dot(Fpx)\n",
    "\n",
    "        # Backtracking line search\n",
    "        t = 1\n",
    "        while F(x + t*delta_x) >= F(x)+alpha*t*Fpx.T.dot(delta_x):\n",
    "            t = beta*t\n",
    "\n",
    "        return x + t*delta_x\n",
    "    \n",
    "    xn = [np.inf, x0]\n",
    "\n",
    "    while np.linalg.norm(xn[-1] - xn[-2]) >= eps:\n",
    "        xn.append(get_next_x(xn[-1]))\n",
    "    \n",
    "    return xn[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barrier method parameter\n",
    "mu = 3\n",
    "\n",
    "def barrier(Fs, Fps, Fpps, x0, eps):\n",
    "    \"\"\"\n",
    "    Approximate the extremum of F using barrier method\n",
    "    (algorithm in slide 80, course 2).\n",
    "    \n",
    "    Args:\n",
    "        Fs: the list with the objective function first and the\n",
    "            inequality constraint functions next\n",
    "        Fps: the list with the gradients of the objective function first\n",
    "             and the inequality constraint functions next\n",
    "        Fpps: the list with the hessians of the objective function first\n",
    "              and the inequality constraint functions next\n",
    "        x0: the initial point\n",
    "        eps: the precision of the approximation\n",
    "    \n",
    "    Returns:\n",
    "        An approximation of the extremum of F.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    t = 1\n",
    "    m = len(Fs) - 1\n",
    "    \n",
    "    while m/t >= eps:\n",
    "        LF, LFp, LFpp = logarithmic_barrier_approx(t, Fs, Fps, Fpps)\n",
    "        x = newton(LF, LFp, LFpp, x, eps)[-1]\n",
    "        t = mu*t\n",
    "    \n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
