{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines solvers\n",
    "\n",
    "The objective is to solve the **primal** problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w \\in \\mathbb{R}^n,\\, z \\in \\mathbb{R}^m}  & \\quad \\frac{1}{2} ||w||_2^2 + C \\mathbb{1}^T z \\\\\n",
    "\\text{subject to } & \\quad y_i(w^T x_i) \\geq 1-z_i, \\quad i = 1,...,m \\\\\n",
    "& \\quad z \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We define $f(w, z) = \\frac{1}{2} ||w||_2^2 + C \\mathbb{1}^T z$ the objective function and, for $i = 1,...,m$, $g_i(w, z) = 1-z_i-y_i(w^T x_i)$ and $h_i(w, z) = -z_i$ the inequality constraint functions.\n",
    "\n",
    "We can rewrite the optimization problem using these functions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,\\, z} & \\quad f(w, z) \\\\\n",
    "\\text{subject to } & \\quad g_i(w, z) \\leq 0, \\quad i = 1,...,m \\\\\n",
    "& \\quad h_i(w, z) \\leq 0, \\quad i = 1,...,m\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note :** A vector $p \\in \\mathbb{R}^k$ is represented by a one-column matrix with $k$ lines. For making notations easy to write, we represent the vertical concatenation of $p \\in \\mathbb{R}^k$ and $q \\in \\mathbb{R}^l$ by $[p \\, |\\, q]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal and dual approximations via logarithmic barrier\n",
    "\n",
    "### Primal approximation\n",
    "\n",
    "We approximate the primal problem by this problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,\\, z} & \\quad f(w, z) - \\frac{1}{t}\\left(\\sum_{i = 1}^m \\log(-g_i(w, z)) +  \\sum_{i = 1}^m \\log(-h_i(w, z))\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To optimize it, we will need the following gradients:\n",
    "\n",
    "- $\\nabla f(w, z) = [w\\,|\\,C\\mathbb{1}]$\n",
    "- $\\nabla g_i(w, z) = -[y_i x_i\\,|\\,\\mathbb{1}_i]$ where $\\mathbb{1}_i$ is the vector with a $1$ at position $i$ and zeros otherwise\n",
    "- $\\nabla h_i(w, z) = -[\\mathbb{0}\\,|\\,\\mathbb{1}_i]$\n",
    "\n",
    "and the following hessians:\n",
    "\n",
    "- $\\nabla^2 f(w, z) = \\left( \\begin{array}{cc}\n",
    "I_n & O \\\\\n",
    "O & O \\end{array} \\right)$\n",
    "- $\\nabla^2 g_i(w, z) = O$\n",
    "- $\\nabla^2 h_i(w, z) = O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_w_z(C, len_w):\n",
    "    \"\"\"\n",
    "    The objective function\n",
    "    p = (w, z)\n",
    "    @type C: int\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return 1/2 * (np.linalg.norm(p[:len_w]))**2 + C * np.ones(p[len_w:, 0].shape).T.dot(p[len_w:, 0])\n",
    "    return f\n",
    "\n",
    "def get_g_i_w_z(i, x, y):\n",
    "    \"\"\"\n",
    "    The inequality constaint on g\n",
    "    p = (w, z)\n",
    "    @param i: a integer beetween 1 and m\n",
    "    @type i: int\n",
    "    @param x: the vector x of the constraint, each value is in R\n",
    "    @type x: int[m][n]\n",
    "    @param y: the vector y of the constraint, each value is in (-1, 1)\n",
    "    @type y: int[m]\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return 1 - p[len(x[0]):, 0][i] - y[i, 0] * p[:len(x[0]), 0].T.dot(x[i])\n",
    "    return f\n",
    "\n",
    "def get_h_i_w_z(i, len_w):\n",
    "    \"\"\"\n",
    "    The inequality constraint on h\n",
    "    p = (w, z)\n",
    "    @param i: a integer beetween 1  and m\n",
    "    @type i: int\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return - p[len_w:, 0][i]\n",
    "    return f\n",
    "\n",
    "def get_primal_fs(C, max_i, x, y):\n",
    "    obj_fun = [get_f_w_z(C, len(x[0]))]\n",
    "    ineq_const_g = [get_g_i_w_z(i, x, y) for i in range(max_i)]\n",
    "    ineq_const_h = [get_h_i_w_z(i, len(x[0])) for i in range(max_i)]\n",
    "    return obj_fun + ineq_const_g + ineq_const_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADIENTS\n",
    "def i_1_vector(i, size):\n",
    "    \"\"\"\n",
    "    Create an array of size \"size\" with 0 everywhere except for the\n",
    "    i-th cell where there is a one\n",
    "    @param i: the cell where a 1 is wanted\n",
    "    @type i: int\n",
    "    @param size: the size of the wanted array\n",
    "    @type size: int\n",
    "    \"\"\"\n",
    "    return np.array([[0.] if k != i else [1.] for k in range(size)])\n",
    "\n",
    "def get_grad_f_w_z(C, len_w):\n",
    "    \"\"\"\n",
    "    The gradient of the objective function\n",
    "    p = (w, z)\n",
    "    @type C: int\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.concatenate((p[:2], C * np.ones(p[len(w):].shape)))\n",
    "    return f\n",
    "\n",
    "def get_grad_g_i_w_z(i, x, y):\n",
    "    \"\"\"\n",
    "    The gradient of the inequality constraint g\n",
    "    p = (w, z)\n",
    "    @param i: a integer beetween 1 and m\n",
    "    @type i: int\n",
    "    @param x: the vector x of the constraint, each value is in R\n",
    "    @type x: int[m][n]\n",
    "    @param y: the vector y of the constraint, each value is in (-1, 1)\n",
    "    @type y: int[m]\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.array(- np.concatenate((y[i] * x[i:i+1].T, i_1_vector(i, len(p[len(x[0]):])))))\n",
    "    return f\n",
    "\n",
    "def get_grad_h_i_w_z(i, len_w):\n",
    "    \"\"\"\n",
    "    The gradient of the inequality constraint h\n",
    "    p = (w, z)\n",
    "    @param i: a integer beetween 1  and m\n",
    "    @type i: int\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.array(- np.concatenate((np.zeros(p[:len_w].shape), i_1_vector(i, len(p[len_w:])))))\n",
    "    return f\n",
    "\n",
    "def get_primal_fps(C, max_i, x, y):\n",
    "    grad_obj_fun = [get_grad_f_w_z(C, len(x[0]))]\n",
    "    grad_ineq_const_g = [get_grad_g_i_w_z(i, x, y) for i in range(max_i)]\n",
    "    grad_ineq_const_h = [get_grad_h_i_w_z(i, len(x[0])) for i in range(max_i)]\n",
    "    return grad_obj_fun + grad_ineq_const_g + grad_ineq_const_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les heissiennes\n",
    "def get_hes_f_w_z(len_w):\n",
    "    \"\"\"\n",
    "    The hessienne of the objective function\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.concatenate((\n",
    "                        np.concatenate((np.identity(len(p[:len_w])), np.zeros((len(p[:len_w]), len(p[len_w:])))), axis = 1), \\\n",
    "                        np.zeros((len(p[len_w:]), len(p[:len_w]) + len(p[len_w:])))))\n",
    "    return f\n",
    "\n",
    "def get_hes_g_i_w_z():\n",
    "    \"\"\"\n",
    "    The heissienne of the inequality constraint g\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.zeros((len(p), len(p)))\n",
    "    return f\n",
    "\n",
    "def get_hes_h_i_w_z():\n",
    "    \"\"\"\n",
    "    The heissienne of the inequality constraint h\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.zeros((len(p), len(p)))\n",
    "    return f\n",
    "\n",
    "def get_primal_fpps(max_i, len_w):\n",
    "    hes_obj_fun = [get_hes_f_w_z(len_w)]\n",
    "    hes_ineq_const_g = [get_hes_g_i_w_z() for _ in range(max_i)]\n",
    "    hes_ineq_const_h = [get_hes_h_i_w_z() for _ in range(max_i)]\n",
    "    return hes_obj_fun + hes_ineq_const_g + hes_ineq_const_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fs_fps_fpps(C, max_i, x, y):\n",
    "    return get_primal_fs(C, max_i, x, y), get_primal_fps(C, max_i, x, y), get_primal_fpps(max_i, len(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual approximation\n",
    "\n",
    "Before approximating the dual, we need to find the **dual** problem. We define the laplacian:\n",
    "$$L(w, z, u, v) = f(w, z) + \\sum_{i = 1}^m u_i g_i(w, z) + \\sum_{i = 1}^m v_i h_i(w, z).$$\n",
    "As $\\nabla^2_{w, z} L(w, z, u, v) = \\left( \\begin{array}{cc}\n",
    "I_n & O \\\\\n",
    "O & O \\end{array} \\right) \\succcurlyeq 0$, $(w, z) \\mapsto L(w, z, u, v)$ is convex. In order to minimize $L(w, z, u, v)$ in $w$ and $z$, we just need to find $w$ and $z$ such that:\n",
    "\n",
    "- $\\nabla_w L(w, z, u, v) = w - \\sum_{i = 1}^{m} u_i y_i x_i = \\mathbb{0}$\n",
    "- $\\nabla_z L(w, z, u, v) = C \\mathbb{1} - u - v = \\mathbb{0}$.\n",
    "\n",
    "Then, we get the dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{u \\in \\mathbb{R}^m}  & \\quad - \\frac{1}{2} ||Ku||_2^2 + \\mathbb{1}^T u \\\\\n",
    "\\text{subject to } & \\quad -u \\leq 0 \\\\\n",
    "& \\quad u - C\\mathbb{1}\\leq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $K = \\left( y_1 x_1 \\,|\\, ... \\,|\\, y_m x_m \\right)$.\n",
    "\n",
    "We define $r(u) = - \\frac{1}{2} ||Ku||_2^2 + \\mathbb{1}^T u$ the objective function and, for $i = 1,...,m$, $s_i(u) = -u_i$ and $t_i(u) = u_i-C$ the inequality constraint functions.\n",
    "\n",
    "We approximate the dual problem by this problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{u} & \\quad r(u) - \\frac{1}{t}\\left(\\sum_{i = 1}^m \\log(-s_i(w, z)) +  \\sum_{i = 1}^m \\log(-t_i(w, z))\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To optimize it, we will need the following gradients:\n",
    "\n",
    "- $\\nabla r(u) = - K^T K u + \\mathbb{1}$\n",
    "- $\\nabla s_i(u) = -\\mathbb{1}_i$\n",
    "- $\\nabla t_i(u) = \\mathbb{1}_i$\n",
    "\n",
    "and the following hessians:\n",
    "\n",
    "- $\\nabla^2 r(u) = - K^T K$\n",
    "- $\\nabla^2 s_i(u) = O$\n",
    "- $\\nabla^2 t_i(u) = O$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r(u) = - \\frac{1}{2} ||Ku||_2^2 + \\mathbb{1}^T u$\n",
    "\n",
    "$s_i(u) = -u_i$ and \n",
    "\n",
    "$t_i(u) = u_i-C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(x, y):\n",
    "    return (x * y).T\n",
    "\n",
    "def get_r_x_y(x, y):\n",
    "    k = K(x, y)\n",
    "    def f(u):\n",
    "        return -1/2 * np.linalg.norm(k.dot(u))**2 + np.sum(u)\n",
    "    return f\n",
    "\n",
    "def get_s_i(i):\n",
    "    def f(u):\n",
    "        return -u[i, 0]\n",
    "    return f\n",
    "\n",
    "def get_t_i(i, c):\n",
    "    def f(u):\n",
    "        return u[i, 0] - c\n",
    "    return f\n",
    "\n",
    "def get_dual_fs(c, max_i, x, y):\n",
    "    obj_fun = [get_r_x_y(x, y)]\n",
    "    ineq_const_s = [get_s_i(i) for i in range(max_i)]\n",
    "    ineq_const_t = [get_t_i(i, c) for i in range(max_i)]\n",
    "    return obj_fun + ineq_const_s + ineq_const_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\nabla r(u) = - K^T K u + \\mathbb{1}$\n",
    "- $\\nabla s_i(u) = -\\mathbb{1}_i$\n",
    "- $\\nabla t_i(u) = \\mathbb{1}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_r_x_y(x, y):\n",
    "    k = np.array(K(x, y))\n",
    "    def f(u):\n",
    "        return - k.T.dot(k).dot(u) + np.ones(u.shape)\n",
    "    return f\n",
    "\n",
    "def get_grad_s_i(i):\n",
    "    def f(u):\n",
    "        return - i_1_vector(i, len(u))\n",
    "    return f\n",
    "\n",
    "def get_grad_t_i(i):\n",
    "    def f(u):\n",
    "        return i_1_vector(1, len(u))\n",
    "    return f\n",
    "\n",
    "def get_dual_fps(max_i, x, y):\n",
    "    grad_obj_fun = [get_grad_r_x_y(x, y)]\n",
    "    grad_ineq_const_s = [get_grad_s_i(i) for i in range(max_i)]\n",
    "    grad_ineq_const_t = [get_grad_t_i(i) for i in range(max_i)]\n",
    "    return grad_obj_fun + grad_ineq_const_s + grad_ineq_const_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\nabla^2 r(u) = - K^T K$\n",
    "- $\\nabla^2 s_i(u) = O$\n",
    "- $\\nabla^2 t_i(u) = O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heis_r_x_y(x, y):\n",
    "    k = np.array(K(x, y))\n",
    "    def f(u):\n",
    "        return - k.T.dot(k)\n",
    "    return f\n",
    "\n",
    "def get_heis_s():\n",
    "    def f(u):\n",
    "        return np.zeros((len(u), len(u)))\n",
    "    return f\n",
    "\n",
    "def get_heis_t():\n",
    "    def f(u):\n",
    "        return np.zeros((len(u), len(u)))\n",
    "    return f\n",
    "\n",
    "def get_dual_fpps(max_i, x, y):\n",
    "    heis_obj_fun = [get_heis_r_x_y(x, y)]\n",
    "    heis_ineq_const_s = [get_heis_s() for _ in range(max_i)]\n",
    "    heis_ineq_const_t = [get_heis_t() for _ in range(max_i)]\n",
    "    return heis_obj_fun + heis_ineq_const_s + heis_ineq_const_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dual_fs_fps_fpps(C, max_i, x, y):\n",
    "    return get_dual_fs(C, max_i, x, y), get_dual_fps(max_i, x, y), get_dual_fpps(max_i, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal and dual optimizations via logarithmic barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logarithmic_barrier_approx(t, Fs, Fps, Fpps):\n",
    "    \"\"\"\n",
    "    Computes the logarithmic barrier approximation function\n",
    "    of a convex optimization problem, its gradient and its hessian\n",
    "    (formulas in slide 74, course 2).\n",
    "    \n",
    "    Args:\n",
    "        t: the approximation quality\n",
    "        Fs: the list with the objective function first and the\n",
    "            inequality constraint functions next\n",
    "        Fps: the list with the gradients of the objective function first\n",
    "             and the inequality constraint functions next\n",
    "        Fpps: the list with the hessians of the objective function first\n",
    "              and the inequality constraint functions next\n",
    "    \n",
    "    Returns:\n",
    "        The logarithmic barrier approximation function, its gradient\n",
    "        and its hessian.\n",
    "    \"\"\"\n",
    "    def LF(x):\n",
    "        r = Fs[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(-np.log(-Fs[i](x)))\n",
    "        return r\n",
    "    \n",
    "    def LFp(x):\n",
    "        r = Fps[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(-1/Fs[i](x)*Fps[i](x))\n",
    "        return r\n",
    "    \n",
    "    def LFpp(x):\n",
    "        r = Fpps[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(1/Fs[i](x)**2*Fps[i](x).dot(Fps[i](x).T)-1/Fs[i](x)*Fpps[i](x))\n",
    "        return r\n",
    "    \n",
    "    return LF, LFp, LFpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Backtracking line search parameters\n",
    "alpha = 1/2\n",
    "beta = 3/4\n",
    "\n",
    "def newton(F, Fp, Fpp, x0, eps):\n",
    "    \"\"\"\n",
    "    Minimizes F using Newton's method (algorithm in slide 20, course 2).\n",
    "    \n",
    "    Args:\n",
    "        F: a function from R^n to R\n",
    "        Fp: the gradient of F\n",
    "        Fpp: the hessian of F\n",
    "        x0: the initial point\n",
    "        eps: the precision of the approximation\n",
    "    \n",
    "    Returns:\n",
    "        The sequence of approximations of the extremum of F up to a\n",
    "        precision eps. Useful for debugging.\n",
    "    \"\"\"\n",
    "    def get_next_x(x):\n",
    "        Fpx = Fp(x)\n",
    "        delta_x = -np.linalg.inv(Fpp(x)).dot(Fpx)\n",
    "        # Backtracking line search$\n",
    "        t = 1\n",
    "        while F(x + t*delta_x) >= F(x)+ alpha * t * Fpx.T.dot(delta_x) or np.isnan(F(x + t*delta_x)):\n",
    "            t = beta*t\n",
    "        return x + t*delta_x\n",
    "        \n",
    "    x_init = np.inf * np.ones((len(x0), 1))\n",
    "    xn = [x_init, x0]\n",
    "    while(True):\n",
    "        λ = Fp(xn[-1]).T.dot(np.linalg.inv(Fpp(xn[-1]))).dot(Fp(xn[-1]))\n",
    "        if λ / 2 <= eps:\n",
    "            return xn[1:]\n",
    "        xn.append(get_next_x(xn[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barrier method parameter\n",
    "mu = 3\n",
    "\n",
    "def barrier(Fs, Fps, Fpps, x0, eps):\n",
    "    \"\"\"\n",
    "    Approximate the extremum of F using barrier method\n",
    "    (algorithm in slide 80, course 2).\n",
    "        Args:\n",
    "        Fs: the list with the objective function first and the\n",
    "            inequality constraint functions next\n",
    "        Fps: the list with the gradients of the objective function first\n",
    "             and the inequality constraint functions next\n",
    "        Fpps: the list with the hessians of the objective function first\n",
    "              and the inequality constraint functions next\n",
    "        x0: the initial point\n",
    "        eps: the precision of the approximation\n",
    "    \n",
    "    Returns:\n",
    "        An approximation of the extremum of F.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    t = 1\n",
    "    m = len(Fs) - 1\n",
    "\n",
    "    \n",
    "    while m/t >= eps:\n",
    "        LF, LFp, LFpp = logarithmic_barrier_approx(t, Fs, Fps, Fpps)\n",
    "        x = newton(LF, LFp, LFpp, x, eps)[-1]\n",
    "        t = mu*t\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f553a7fb940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on prepare les datasets\n",
    "x_ok = np.random.multivariate_normal([-3, -3], [[1, 0], [0, 1]], size = 1)\n",
    "y_ok = np.array([[-1] for _ in range(1)])\n",
    "\n",
    "x_ko = np.random.multivariate_normal([3, 3], [[1, 0], [0, 1]], size = 1)\n",
    "y_ko = np.array([[1] for _ in range(1)])\n",
    "\n",
    "plt.scatter(x_ok[:, 0], x_ok[:, 1], color = \"red\")\n",
    "plt.scatter(x_ko[:, 0], x_ko[:, 1], color = \"blue\")\n",
    "plt.show()\n",
    "\n",
    "x = np.concatenate((x_ok, x_ko))\n",
    "y = np.concatenate((y_ok, y_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, fps, fpps = get_fs_fps_fpps(3.0, len(x), x, y)\n",
    "x_0 = np.ones((len(x[0]) + len(x), 1))\n",
    "\n",
    "b = barrier(fs, fps, fpps, x_0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x0):\n",
    "    return -b[0]*x0/b[1]\n",
    "\n",
    "plt.scatter(x_ok[:, 0], x_ok[:, 1], color = \"red\")\n",
    "plt.scatter(x_ko[:, 0], x_ko[:, 1], color = \"blue\")\n",
    "plt.plot([-4, 0, 4], f(np.array([-4, 0, 4])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, fps, fpps = get_dual_fs_fps_fpps(2.0, len(x), x, y)\n",
    "u = np.ones((len(x), 1))\n",
    "b = barrier(fs, fps, fpps, u, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "a = sklearn.svm.SVC(C = 4.0)\n",
    "a.fit(x, y)\n",
    "\n",
    "a.score(x, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
