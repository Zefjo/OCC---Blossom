{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines solvers\n",
    "\n",
    "The objective is to solve the **primal** problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w \\in \\mathbb{R}^n,\\, z \\in \\mathbb{R}^m}  & \\quad \\frac{1}{2} ||w||_2^2 + C \\mathbb{1}^T z \\\\\n",
    "\\text{subject to } & \\quad y_i(w^T x_i) \\geq 1-z_i, \\quad i = 1,...,m \\\\\n",
    "& \\quad z \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We define $f(w, z) = \\frac{1}{2} ||w||_2^2 + C \\mathbb{1}^T z$ the objective function and, for $i = 1,...,m$, $g_i(w, z) = 1-z_i-y_i(w^T x_i)$ and $h_i(w, z) = -z_i$ the inequality constraint functions.\n",
    "\n",
    "We can rewrite the optimization problem using these functions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,\\, z} & \\quad f(w, z) \\\\\n",
    "\\text{subject to } & \\quad g_i(w, z) \\leq 0, \\quad i = 1,...,m \\\\\n",
    "& \\quad h_i(w, z) \\leq 0, \\quad i = 1,...,m\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note :** A vector $p \\in \\mathbb{R}^k$ is represented by a one-column matrix with $k$ lines. For making notations easy to write, we represent the vertical concatenation of $p \\in \\mathbb{R}^k$ and $q \\in \\mathbb{R}^l$ by $[p \\, |\\, q]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal and dual approximations via logarithmic barrier\n",
    "\n",
    "### Primal approximation\n",
    "\n",
    "We approximate the primal problem by this problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,\\, z} & \\quad f(w, z) - \\frac{1}{t}\\left(\\sum_{i = 1}^m \\log(-g_i(w, z)) +  \\sum_{i = 1}^m \\log(-h_i(w, z))\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To optimize it, we will need the following gradients:\n",
    "\n",
    "- $\\nabla f(w, z) = [w\\,|\\,C\\mathbb{1}]$\n",
    "- $\\nabla g_i(w, z) = -[y_i x_i\\,|\\,\\mathbb{1}_i]$ where $\\mathbb{1}_i$ is the vector with a $1$ at position $i$ and zeros otherwise\n",
    "- $\\nabla h_i(w, z) = -[\\mathbb{0}\\,|\\,\\mathbb{1}_i]$\n",
    "\n",
    "and the following hessians:\n",
    "\n",
    "- $\\nabla^2 f(w, z) = \\left( \\begin{array}{cc}\n",
    "I_n & O \\\\\n",
    "O & O \\end{array} \\right)$\n",
    "- $\\nabla^2 g_i(w, z) = O$\n",
    "- $\\nabla^2 h_i(w, z) = O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_w_z(C, len_w):\n",
    "    \"\"\"\n",
    "    The objective function\n",
    "    p = (w, z)\n",
    "    @type C: int\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return 1/2 * (np.linalg.norm(p[:len_w]))**2 + C * np.ones(p[len_w:, 0].shape).T.dot(p[len_w:, 0])\n",
    "    return f\n",
    "\n",
    "def get_g_i_w_z(i, x, y):\n",
    "    \"\"\"\n",
    "    The inequality constaint on g\n",
    "    p = (w, z)\n",
    "    @param i: a integer beetween 1 and m\n",
    "    @type i: int\n",
    "    @param x: the vector x of the constraint, each value is in R\n",
    "    @type x: int[m][n]\n",
    "    @param y: the vector y of the constraint, each value is in (-1, 1)\n",
    "    @type y: int[m]\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return 1 - p[len(x[0]):, 0][i] - y[i, 0] * p[:len(x[0]), 0].T.dot(x[i])\n",
    "    return f\n",
    "\n",
    "def get_h_i_w_z(i, len_w):\n",
    "    \"\"\"\n",
    "    The inequality constraint on h\n",
    "    p = (w, z)\n",
    "    @param i: a integer beetween 1  and m\n",
    "    @type i: int\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return - p[len_w:, 0][i]\n",
    "    return f\n",
    "\n",
    "def get_primal_fs(C, max_i, x, y):\n",
    "    obj_fun = [get_f_w_z(C, len(x[0]))]\n",
    "    ineq_const_g = [get_g_i_w_z(i, x, y) for i in range(max_i)]\n",
    "    ineq_const_h = [get_h_i_w_z(i, len(x[0])) for i in range(max_i)]\n",
    "    return obj_fun + ineq_const_g + ineq_const_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADIENTS\n",
    "def i_1_vector(i, size):\n",
    "    \"\"\"\n",
    "    Create an array of size \"size\" with 0 everywhere except for the\n",
    "    i-th cell where there is a one\n",
    "    @param i: the cell where a 1 is wanted\n",
    "    @type i: int\n",
    "    @param size: the size of the wanted array\n",
    "    @type size: int\n",
    "    \"\"\"\n",
    "    return np.array([[0.] if k != i else [1.] for k in range(size)])\n",
    "\n",
    "def get_grad_f_w_z(C, len_w):\n",
    "    \"\"\"\n",
    "    The gradient of the objective function\n",
    "    p = (w, z)\n",
    "    @type C: int\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.concatenate((p[:2], C * np.ones(p[len(w):].shape)))\n",
    "    return f\n",
    "\n",
    "def get_grad_g_i_w_z(i, x, y):\n",
    "    \"\"\"\n",
    "    The gradient of the inequality constraint g\n",
    "    p = (w, z)\n",
    "    @param i: a integer beetween 1 and m\n",
    "    @type i: int\n",
    "    @param x: the vector x of the constraint, each value is in R\n",
    "    @type x: int[m][n]\n",
    "    @param y: the vector y of the constraint, each value is in (-1, 1)\n",
    "    @type y: int[m]\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.array(- np.concatenate((y[i] * x[i:i+1].T, i_1_vector(i, len(p[len(x[0]):])))))\n",
    "    return f\n",
    "\n",
    "def get_grad_h_i_w_z(i, len_w):\n",
    "    \"\"\"\n",
    "    The gradient of the inequality constraint h\n",
    "    p = (w, z)\n",
    "    @param i: a integer beetween 1  and m\n",
    "    @type i: int\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.array(- np.concatenate((np.zeros(p[:len_w].shape), i_1_vector(i, len(p[len_w:])))))\n",
    "    return f\n",
    "\n",
    "def get_primal_fps(C, max_i, x, y):\n",
    "    grad_obj_fun = [get_grad_f_w_z(C, len(x[0]))]\n",
    "    grad_ineq_const_g = [get_grad_g_i_w_z(i, x, y) for i in range(max_i)]\n",
    "    grad_ineq_const_h = [get_grad_h_i_w_z(i, len(x[0])) for i in range(max_i)]\n",
    "    return grad_obj_fun + grad_ineq_const_g + grad_ineq_const_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les heissiennes\n",
    "def get_hes_f_w_z(len_w):\n",
    "    \"\"\"\n",
    "    The hessienne of the objective function\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.concatenate((\n",
    "                        np.concatenate((np.identity(len(p[:len_w])), np.zeros((len(p[:len_w]), len(p[len_w:])))), axis = 1), \\\n",
    "                        np.zeros((len(p[len_w:]), len(p[:len_w]) + len(p[len_w:])))))\n",
    "    return f\n",
    "\n",
    "def get_hes_g_i_w_z():\n",
    "    \"\"\"\n",
    "    The heissienne of the inequality constraint g\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.zeros((len(p), len(p)))\n",
    "    return f\n",
    "\n",
    "def get_hes_h_i_w_z():\n",
    "    \"\"\"\n",
    "    The heissienne of the inequality constraint h\n",
    "    \"\"\"\n",
    "    def f(p):\n",
    "        return np.zeros((len(p), len(p)))\n",
    "    return f\n",
    "\n",
    "def get_primal_fpps(max_i, len_w):\n",
    "    hes_obj_fun = [get_hes_f_w_z(len_w)]\n",
    "    hes_ineq_const_g = [get_hes_g_i_w_z() for _ in range(max_i)]\n",
    "    hes_ineq_const_h = [get_hes_h_i_w_z() for _ in range(max_i)]\n",
    "    return hes_obj_fun + hes_ineq_const_g + hes_ineq_const_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fs_fps_fpps(C, max_i, x, y):\n",
    "    return get_primal_fs(C, max_i, x, y), get_primal_fps(C, max_i, x, y), get_primal_fpps(max_i, len(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual approximation\n",
    "\n",
    "Before approximating the dual, we need to find the **dual** problem. We define the laplacian:\n",
    "$$L(w, z, u, v) = f(w, z) + \\sum_{i = 1}^m u_i g_i(w, z) + \\sum_{i = 1}^m v_i h_i(w, z).$$\n",
    "As $\\nabla^2_{w, z} L(w, z, u, v) = \\left( \\begin{array}{cc}\n",
    "I_n & O \\\\\n",
    "O & O \\end{array} \\right) \\succcurlyeq 0$, $(w, z) \\mapsto L(w, z, u, v)$ is convex. In order to minimize $L(w, z, u, v)$ in $w$ and $z$, we just need to find $w$ and $z$ such that:\n",
    "\n",
    "- $\\nabla_w L(w, z, u, v) = w - \\sum_{i = 1}^{m} u_i y_i x_i = \\mathbb{0}$\n",
    "- $\\nabla_z L(w, z, u, v) = C \\mathbb{1} - u - v = \\mathbb{0}$.\n",
    "\n",
    "Then, we get the dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{u \\in \\mathbb{R}^m}  & \\quad - \\frac{1}{2} ||Ku||_2^2 + \\mathbb{1}^T u \\\\\n",
    "\\text{subject to } & \\quad -u \\leq 0 \\\\\n",
    "& \\quad u - C\\mathbb{1}\\leq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $K = \\left( y_1 x_1 \\,|\\, ... \\,|\\, y_m x_m \\right)$.\n",
    "\n",
    "We define $r(u) = - \\frac{1}{2} ||Ku||_2^2 + \\mathbb{1}^T u$ the objective function and, for $i = 1,...,m$, $s_i(u) = -u_i$ and $t_i(u) = u_i-C$ the inequality constraint functions.\n",
    "\n",
    "We approximate the dual problem by this problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{u} & \\quad r(u) - \\frac{1}{t}\\left(\\sum_{i = 1}^m \\log(-s_i(w, z)) +  \\sum_{i = 1}^m \\log(-t_i(w, z))\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To optimize it, we will need the following gradients:\n",
    "\n",
    "- $\\nabla r(u) = - K^T K u + \\mathbb{1}$\n",
    "- $\\nabla s_i(u) = -\\mathbb{1}_i$\n",
    "- $\\nabla t_i(u) = \\mathbb{1}_i$\n",
    "\n",
    "and the following hessians:\n",
    "\n",
    "- $\\nabla^2 r(u) = - K^T K$\n",
    "- $\\nabla^2 s_i(u) = O$\n",
    "- $\\nabla^2 t_i(u) = O$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r(u) = - \\frac{1}{2} ||Ku||_2^2 + \\mathbb{1}^T u$\n",
    "\n",
    "$s_i(u) = -u_i$ and \n",
    "\n",
    "$t_i(u) = u_i-C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(x, y):\n",
    "    return x * y\n",
    "\n",
    "def get_r_x_y(x, y):\n",
    "    k = K(x, y)\n",
    "    def f(u):\n",
    "        return -1/2 * np.linalg.norm(k * u)**2 + np.sum(u)\n",
    "    return f\n",
    "\n",
    "def get_s_i(i):\n",
    "    def f(u):\n",
    "        return -u[i, 0]\n",
    "    return f\n",
    "\n",
    "def get_t_i(i, c):\n",
    "    def f(u):\n",
    "        return u[i, 0] - c\n",
    "    return f\n",
    "\n",
    "def get_dual_fs(c, max_i, x, y):\n",
    "    obj_fun = [get_r_x_y(x, y)]\n",
    "    ineq_const_s = [get_s_i(i) for i in range(max_i)]\n",
    "    ineq_const_t = [get_t_i(i, c) for i in range(max_i)]\n",
    "    return obj_fun + ineq_const_s + ineq_const_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\nabla r(u) = - K^T K u + \\mathbb{1}$\n",
    "- $\\nabla s_i(u) = -\\mathbb{1}_i$\n",
    "- $\\nabla t_i(u) = \\mathbb{1}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_r_x_y(x, y):\n",
    "    k = np.array(K(x, y))\n",
    "    def f(u):\n",
    "        return - k.dot(k.T).dot(u) + np.ones(u.shape)\n",
    "    return f\n",
    "\n",
    "def get_grad_s_i(i):\n",
    "    def f(u):\n",
    "        return - i_1_vector(i, len(u))\n",
    "    return f\n",
    "\n",
    "def get_grad_t_i(i):\n",
    "    def f(u):\n",
    "        return i_1_vector(1, len(u))\n",
    "    return f\n",
    "\n",
    "def get_dual_fps(max_i, x, y):\n",
    "    grad_obj_fun = [get_grad_r_x_y(x, y)]\n",
    "    grad_ineq_const_s = [get_grad_s_i(i) for i in range(max_i)]\n",
    "    grad_ineq_const_t = [get_grad_t_i(i) for i in range(max_i)]\n",
    "    return grad_obj_fun + grad_ineq_const_s + grad_ineq_const_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\nabla^2 r(u) = - K^T K$\n",
    "- $\\nabla^2 s_i(u) = O$\n",
    "- $\\nabla^2 t_i(u) = O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heis_r_x_y(x, y):\n",
    "    k = np.array(K(x, y))\n",
    "    def f(u):\n",
    "        return - k.dot(k.T)\n",
    "    return f\n",
    "\n",
    "def get_heis_s():\n",
    "    def f(u):\n",
    "        return np.zeros((len(u), len(u)))\n",
    "    return f\n",
    "\n",
    "def get_heis_t():\n",
    "    def f(u):\n",
    "        return np.zeros((len(u), len(u)))\n",
    "    return f\n",
    "\n",
    "def get_dual_fpps(max_i, x, y):\n",
    "    heis_obj_fun = [get_heis_r_x_y(x, y)]\n",
    "    heis_ineq_const_s = [get_heis_s() for _ in range(max_i)]\n",
    "    heis_ineq_const_t = [get_heis_t() for _ in range(max_i)]\n",
    "    return heis_obj_fun + heis_ineq_const_s + heis_ineq_const_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dual_fs_fps_fpps(C, max_i, x, y):\n",
    "    return get_dual_fs(C, max_i, x, y), get_dual_fps(max_i, x, y), get_dual_fpps(max_i, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal and dual optimizations via logarithmic barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logarithmic_barrier_approx(t, Fs, Fps, Fpps):\n",
    "    \"\"\"\n",
    "    Computes the logarithmic barrier approximation function\n",
    "    of a convex optimization problem, its gradient and its hessian\n",
    "    (formulas in slide 74, course 2).\n",
    "    \n",
    "    Args:\n",
    "        t: the approximation quality\n",
    "        Fs: the list with the objective function first and the\n",
    "            inequality constraint functions next\n",
    "        Fps: the list with the gradients of the objective function first\n",
    "             and the inequality constraint functions next\n",
    "        Fpps: the list with the hessians of the objective function first\n",
    "              and the inequality constraint functions next\n",
    "    \n",
    "    Returns:\n",
    "        The logarithmic barrier approximation function, its gradient\n",
    "        and its hessian.\n",
    "    \"\"\"\n",
    "    def LF(x):\n",
    "        r = Fs[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(-np.log(-Fs[i](x)))\n",
    "        return r\n",
    "    \n",
    "    def LFp(x):\n",
    "        r = Fps[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(-1/Fs[i](x)*Fps[i](x))\n",
    "        return r\n",
    "    \n",
    "    def LFpp(x):\n",
    "        r = Fpps[0](x)\n",
    "        for i in range(1, len(Fs)):\n",
    "            r += 1/t*(1/Fs[i](x)**2*Fps[i](x).dot(Fps[i](x).T)-1/Fs[i](x)*Fpps[i](x))\n",
    "        return r\n",
    "    \n",
    "    return LF, LFp, LFpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Backtracking line search parameters\n",
    "alpha = 1/2\n",
    "beta = 3/4\n",
    "\n",
    "def newton(F, Fp, Fpp, x0, eps):\n",
    "    \"\"\"\n",
    "    Minimizes F using Newton's method (algorithm in slide 20, course 2).\n",
    "    \n",
    "    Args:\n",
    "        F: a function from R^n to R\n",
    "        Fp: the gradient of F\n",
    "        Fpp: the hessian of F\n",
    "        x0: the initial point\n",
    "        eps: the precision of the approximation\n",
    "    \n",
    "    Returns:\n",
    "        The sequence of approximations of the extremum of F up to a\n",
    "        precision eps. Useful for debugging.\n",
    "    \"\"\"\n",
    "    def get_next_x(x):\n",
    "        Fpx = Fp(x)\n",
    "        delta_x = -np.linalg.inv(Fpp(x)).dot(Fpx)\n",
    "        # Backtracking line search$\n",
    "        t = 1\n",
    "        while F(x + t*delta_x) >= F(x)+ alpha * t * Fpx.T.dot(delta_x) or np.isnan(F(x + t*delta_x)):\n",
    "            t = beta*t\n",
    "        return x + t*delta_x\n",
    "        \n",
    "    x_init = np.inf * np.ones((len(x0), 1))\n",
    "    xn = [x_init, x0]\n",
    "    while(True):\n",
    "        λ = Fp(xn[-1]).T.dot(np.linalg.inv(Fpp(xn[-1]))).dot(Fp(xn[-1]))\n",
    "        if λ / 2 <= eps:\n",
    "            return xn[1:]\n",
    "        xn.append(get_next_x(xn[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barrier method parameter\n",
    "mu = 3\n",
    "\n",
    "def barrier(Fs, Fps, Fpps, x0, eps):\n",
    "    \"\"\"\n",
    "    Approximate the extremum of F using barrier method\n",
    "    (algorithm in slide 80, course 2).\n",
    "        Args:\n",
    "        Fs: the list with the objective function first and the\n",
    "            inequality constraint functions next\n",
    "        Fps: the list with the gradients of the objective function first\n",
    "             and the inequality constraint functions next\n",
    "        Fpps: the list with the hessians of the objective function first\n",
    "              and the inequality constraint functions next\n",
    "        x0: the initial point\n",
    "        eps: the precision of the approximation\n",
    "    \n",
    "    Returns:\n",
    "        An approximation of the extremum of F.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    t = 1\n",
    "    m = len(Fs) - 1\n",
    "\n",
    "    \n",
    "    while m/t >= eps:\n",
    "        LF, LFp, LFpp = logarithmic_barrier_approx(t, Fs, Fps, Fpps)\n",
    "        x = newton(LF, LFp, LFpp, x, eps)[-1]\n",
    "        t = mu*t\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on prepare les datasets\n",
    "x_ok = np.random.multivariate_normal([-3, -3], [[1, 0], [0, 1]], size = 100)\n",
    "y_ok = np.array([[-1] for _ in range(100)])\n",
    "\n",
    "x_ko = np.random.multivariate_normal([3, 3], [[1, 0], [0, 1]], size = 100)\n",
    "y_ko = np.array([[1] for _ in range(100)])\n",
    "\n",
    "plt.scatter(x_ok[:, 0], x_ok[:, 1], color = \"red\")\n",
    "plt.scatter(x_ko[:, 0], x_ko[:, 1], color = \"blue\")\n",
    "plt.show()\n",
    "\n",
    "x = np.concatenate((x_ok, x_ko))\n",
    "y = np.concatenate((y_ok, y_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, fps, fpps = get_fs_fps_fpps(1.0, len(x), x, y)\n",
    "x_0 = np.ones((len(x[0]) + len(x), 1))\n",
    "\n",
    "b = barrier(fs, fps, fpps, x_0, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x0):\n",
    "    return -b[0]*x0/b[1]\n",
    "\n",
    "plt.scatter(x_ok[:, 0], x_ok[:, 1], color = \"red\")\n",
    "plt.scatter(x_ko[:, 0], x_ko[:, 1], color = \"blue\")\n",
    "plt.plot([-4, 0, 4], f(np.array([-4, 0, 4])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "a = sklearn.svm.SVC(C = 4.0)\n",
    "a.fit(x, y)\n",
    "\n",
    "a.score(x, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
